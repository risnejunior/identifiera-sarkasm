# Identifiera sarkasm i text
Kandidatarbete 2017

## Folder structure for datasets
```
.
â””â”€â”€ datasets
Â Â  â”œâ”€â”€ glove_twitter_embeddings
Â Â  â”œâ”€â”€ imdb
Â Â  â”‚Â Â  â”œâ”€â”€ neg
Â Â  â”‚Â Â  â””â”€â”€ pos
Â Â  â””â”€â”€ poria
Â Â      â”œâ”€â”€ en-balanced
Â Â      â”‚Â Â  â”œâ”€â”€ neg
Â Â      â”‚Â Â  â””â”€â”€ pos
Â Â      â””â”€â”€ en-ratio
Â Â        Â  â”œâ”€â”€ neg
Â Â        Â  â””â”€â”€ pos
```

## Get the data

Right now the following data is used:
- Glove twitter embeddings
- The downloaded tweets
- doImdb reviews


1. Download the imdb & twitter data (datasets.zip in this folder)
2. Unzip the imdb & twitter data in: `identifiera-sarkasm\datasets`, overwrite any existing data. Make sure you **donâ€™t** get this structure: `\identifiera-sarkasm\datasets\datasets\`
3. You should have these three folders inside the dataset directory (you might already have others):
  1. poria
  2. Imdb
  3. glove_twitter_embeddings
4. Download the glove twitter embeddings:http://nlp.stanford.edu/data/glove.twitter.27B.zip (might take a while so feel free to copy from friend)
5. Unzip the embeddings inside the â€˜glove_twitter_embeddingsâ€™ directory (4 files).
6. Profit!

## Description of the source files
The following python files are in the source directory:

* **analyze.py**: just a stub right now, meant to load an already trained network
* **clean_tweets.py**: cleans the tweets from links, hashtags etc
* **scraper.py**: downloads tweets, you probably wonâ€™t need this as they already are downloaded.
* **common_funs.py**: library file containing common functions used in other files
* **preprocess_data.py**: tokenizes the tweets and turns the data into a form ready to be used in the network.
* **tflearn_rnn.py**: the code that runs the classifier
* **twokenize.py**: a twitter tokenization library, unused for now
## What the code does

1. The scraper reads the tweet idâ€™s from normal.txt and sarcastic.txt, downloads these tweets and puts them in balanced_normal_tweets.csv and balanced_sarcastic_tweets.csv
2. The clean_tweets code takes the tweets from the aforementioned .csv files, tokenizes & cleans the tweets, and then puts them in the cleaned folder, one file per tweet.
3. Preprocess_data reads the cleaned tweets and creates 4 .json files:
  1. The **vocabulary**: it contains the n-most common words from the tweets, with the words mapped to an integer. â€˜.â€™ maps to 0, and â€˜_â€™ maps to 1. â€˜_â€™ signifies words missing in the dictionary. â€˜.â€™ signifies the padding at the end of tweets.
  2. The **reverse vocabulary**: itâ€™s the same mapping as in the vocabulary just reversed
  3. The **preprocesses_data** file: it contains all tweets in the form, each with 3 fields: the tweet id, a list of the tokenized words from the tweet, and a list of the mappings of this words according to the dictionary
  4. The **embeddings_file**: itâ€™s a list of all the world vectors, sorted in the same order as the vocabulary file. So if a word has index n in the vocabulary itâ€™s corresponding word vector will be on row n in the embeddings file.
4. **Tflearn_rnn.py**: takes the .json files the preprocessed data and embeddings, and trains the network with these. In the end it prints a confusion matrix for the training, evaluation and test set.

## How to run the code the first time
1. If you already have the data downloaded you first need to run the clean_tweets.py file. Unless the cleaning strategy changes you only need to do this once.
2. Run preprocess_data.py, if you get an error message regarding missing stopwords, change print_debug to False. You can also try to write this in console to download the dependencies:
  1. Import nltk
  2. nltk.download()
3. Run **tflearn_rnn.py**

## Settings
Settings are located in the settings.py file and shared by the other scripts

## Unicode issues (probably only windows users)
* If you have problems writing utf-8 to console, i.e some error about unicode mapping, type this in console, â€˜chcp 65001 & cmdâ€™, just after starting the console (windows only)
* If your console displays ?-signs or has similar problems displaying certain chars try changing the font to one that has better utf-8 support, like â€˜dejavu mono sansâ€™.
* In ConEMu (3rd party console) you can set this up to work automatically by going to settings->main and change alternative fonts to: â€˜DejaVu Sans Monoâ€™. Then go to startup->environment and add â€˜chcp utf8â€™ to the environment variables (under PATH).

## Tensorboard
To run Tensorboard write this in console: `tensorboard --logdir=/tmp/tflearn_logs/`

Tensorboard will start a web server and print out the address where you can find it using your web browser.

## Roadmap ğŸš
- [ ] Skriva lite kod
- [ ] Identifiera sarkasm i text
- [ ] ğŸº
- [ ] gÃ¶ra om preprocess sÃ¥ att den sparar data i .npy format, gÃ¶r sÃ¥ att 
        tflearn_rnn inte mÃ¥ste gÃ¶ra om nÃ¥gon data
- [ ] Ã¤ndra sÃ¥ att <hashtag> kommer med i vokabulÃ¤ret (endast icke sarkastiska) 
- [ ] felsÃ¶ka embeddings sÃ¥ att de stÃ¤mmer Ã¶verens med vokabulÃ¤ret 
- [ ] Ã¤ndra padding och placholder fÃ¶r embeddings. placeholder ska vara 
        random och padding kanske 0?
- [ ] bygga en 'trining director' automatiskt trÃ¤nar med olika hyperparametrar
        och sen sparar resultatet.
- [ ] GÃ¶ra sÃ¥ att tensorboard skriver ut tvÃ¥ grafer i samma ruta under samma
        trÃ¤ningspass, gÃ¥r sedan att anvÃ¤na till att diganosiera overfitting:
          http://ischlag.github.io/2016/06/04/how-to-use-tensorboard/
- [ ] visualisera embeddings i tensorboard enligt: 
        https://www.tensorflow.org/get_started/embedding_viz
- [ ] Skriva om preprocessing fÃ¶r embeddings sÃ¥ det inte tar sÃ¥ mkt minne,
        omÃ¶jligt just nu at kÃ¶ra 200-embeddings med 16GB ram